# Great Deep Learning Tutorials for Natural Language Processing (NLP)
A Great Collection of Deep Learning Tutorials and Repositories for Natural Language Processing (NLP)

## General:
- [Great NLP Posts](http://jalammar.github.io/)  
- [Awesome NLP Paper Discussions - Hugging Face](https://github.com/huggingface/awesome-papers) [_Excellent_]  
- [Ten trends in Deep learning NLP](https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/)  
- [Attention in RNNs](https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05)  
- [BERT - TensorFlow](https://github.com/google-research/bert)  
- [Understanding XLNet](https://www.borealisai.com/en/blog/understanding-xlnet/)  
- [XLNet - TensorFlow](https://github.com/zihangdai/xlnet)  
- [XLM (PyTorch implementation of Cross-lingual Language Model Pretraining)](https://github.com/facebookresearch/XLM)  
- [Pretrained PyTorch models for BERT](https://github.com/huggingface/pytorch-pretrained-BERT)  
- [Library of state-of-the-art pretrained models for NLP](https://github.com/huggingface/pytorch-transformers#quick-tour) [_Excellent_]  
- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5)  
- [PyTorch Hub - BERT](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)  
- [A Simple Guide On Using BERT for Binary Text Classification](https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04)  
- [Core ML 3 implementation of BERT for Question answering](https://github.com/huggingface/swift-coreml-transformers)  
- [NLP - Keras - Intro](https://nlpforhackers.io/keras-intro/)  
- [AllenNLP](https://allennlp.org/)  [_General NLP_]
- [Stanza - A Python NLP Library for Many Human Languages](https://stanfordnlp.github.io/stanza/)  
- [The Best NLP Papers From ICLR 2020](https://www.topbots.com/best-nlp-papers-from-iclr-2020)  
- [Deep learning for natural language processing and information retrieval at the University of Waterloo](https://github.com/castorini)  
- [Natural Language Processing With spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)  [_Great_]  
- [NLP Papers](https://github.com/AliAkbarBadri/nlp-papers)   
- [A Great NLP Course](https://lena-voita.github.io/nlp_course.html)  
- [KerasNLP: Modular NLP Workflows for Keras](https://github.com/keras-team/keras-nlp)  
- [NLP Test: Deliver Safe & Effective Models](https://github.com/JohnSnowLabs/nlptest)  

## General Persian based libraries:
- [Parsivar: library for Persian text preprocessing](https://github.com/ICTRC/Parsivar)   
- [Hazm](https://github.com/sobhe/hazm)  
- [persianNLP](https://github.com/persiannlp)  
- [ParsiNLU: Comprehensive suit of high-level NLP tasks for Persian language](https://github.com/persiannlp/parsinlu)   
- [FarsTail: A Persian Natural Language Inference Dataset](https://github.com/dml-qom/FarsTail)  
- [wordfreq: Access a database of word frequencies](https://github.com/rspeer/wordfreq)  
- [Persian Stop Words List](https://github.com/kharazi/persian-stopwords)  
- [Persian Stop Words List in Hazm Repo](https://github.com/sobhe/hazm/blob/master/hazm/data/stopwords.dat)  

## Text Representation:
- [Beyond Word Embeddings Part 1](https://towardsdatascience.com/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f)  
- [Beyond Word Embeddings Part 2](https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec)  
- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)  
- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)  
- [Word Embedding](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)  
- [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)  
- [Introduction to Word Vectors](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf) 
- [Word2vec Made Easy](https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae)  
- [What is GloVe? Part I](https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970)  
- [What is GloVe? Part II](https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0)  
- [What is GloVe? Part III](https://towardsdatascience.com/emnlp-what-is-glove-part-iii-c6090bed114)  
- [What is GloVe? Part IV](https://towardsdatascience.com/emnlp-what-is-glove-part-iv-e605a4c407c8)  
- [What is GloVe? Part V](https://towardsdatascience.com/emnlp-what-is-glove-part-v-fa888272c290)  
- [ELMo: Deep Contextualized Word Representation](https://allennlp.org/elmo)  
- [A Step-by-Step NLP Guide to Learn ELMo](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)  
- [ELMo: Contextual language embedding](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)  
- [word embeddings with ELMo](https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045)  
- [Doc2Vec - Gensim](https://radimrehurek.com/gensim/models/doc2vec.html)  

## Self-Supervised Learning in NLP:
- [https://amitness.com/2020/05/self-supervised-learning-nlp/](https://amitness.com/2020/05/self-supervised-learning-nlp/)  
- [COSINE: Fine-Tuning Pre-trained Language Model with Weak Supervision](https://github.com/yueyu1030/COSINE)  

## RNN, LSTM, and GRU:
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- [Illustrated Guide to LSTM’s and GRU’s](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)  
- [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)  
- [Recurrent Neural Networks and LSTM explained](https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9)  
- [Long Short-Term Memory (LSTM): Concept](https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359)  
- [Understanding architecture of LSTM cell from scratch](https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4)  
- [Basic understanding of LSTM](https://blog.goodaudience.com/basic-understanding-of-lstm-539f3b013f1e)  
- [Taming LSTMs with PyTorch](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e)  
- [Introduction to LSTM](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_medium=ELMoNLParticle&utm_source=blog)  
- [Introduction to RNNs](https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/)  

## Transformers:
- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)  
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  
- [What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)  
- [How Transformers work in deep learning and NLP](https://theaisummer.com/transformer/)    
- [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)  
- [How do Transformers Work in NLP?](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)  
- [The Essence of Transformers](https://towardsdatascience.com/the-essence-of-transformers-9fb8e14cc465) [Good]    
- [Transformers and Multi Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)  
- [Multi Head Attention](https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html)  
- [BERT for Dummies](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)  
- [The Dark Secrets of BERT](https://text-machine-lab.github.io/blog/2020/bert-secrets/)  
- [A Survey of Long-Term Context in Transformers](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/) [_Great_]  
- [The Transformer Family](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)  
- [The Transformer Isn’t As Hard To Understand As You Might Think](https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8)  
- [Review of Compact Transformer Architectures](https://medium.com/@jfd2139/review-of-compact-transformer-architectures-c477b797e2d5) [**Great**]   
- [REFORMER: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)  
- [GPT-3: Language Models are Few-Shot Learners](https://github.com/openai/gpt-3)  
- [GPT-3 Sandbox](https://github.com/shreyashankar/gpt3-sandbox)  
- [Microsoft will launch GPT-4](https://medium.com/@yablonassaf/microsoft-will-launch-gpt-4-with-ai-videos-on-wednesday-75d882e0260e)  
- [OpenAI GPT-4](https://openai.com/research/gpt-4)  
- [Some information about GPT-4](https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-artificialintelligence-activity-7041793426530390016-5P-n/?utm_source=share&utm_medium=member_android)  
- [Regular Expressions (Regex) Generated by GPT-3](https://losslesshq.com/)  
- [Auto Regex: Converting English description to Regex](https://www.autoregex.xyz/) [Good]  
- [minGPT](https://github.com/karpathy/minGPT)  
- [NVIDIA FasterTransformer: Transformer related optimization, including BERT & GPT](https://github.com/NVIDIA/FasterTransformer)  
- [OpenNMT CTranslate2: Fast inference engine for Transformer models](https://github.com/OpenNMT/CTranslate2/)  
- [Deploying GPT-J and T5 with FasterTransformer and Triton Inference Server](https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/?ncid=so-link-499508#cid=dl05_so-link_en-us) [Interesting]  
- [MEND: Fast Model Editing at Scale](https://github.com/eric-mitchell/mend) [**Excellent Work**]   
- [BorealisAI Transformers I: Introduction](https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/)  
- [OpenAI Best Practices for Deploying Language Models](https://openai.com/blog/best-practices-for-deploying-language-models/)  
- [OPT-IML](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML)  
- [Transformer Taxonomy](https://kipp.ly/blog/transformer-taxonomy/) [Great]  

### Reinforcement Learning from Human Feedback (RLHF):
- [RLHF Tutorial](https://vinija.ai/concepts/RLHF/)  

### Large Language Models (LLMs):
- [LLaMA](https://github.com/facebookresearch/llama)  
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761) [Great]  
- [Toolformer GitHub](https://github.com/lucidrains/toolformer-pytorch)  
- [Amazon Multimodal Chain-of-Thought Reasoning in Language Models](https://github.com/amazon-science/mm-cot)  
- [LLaMA-based ChatGPT Training](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama) [Great]  
- [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/abs/2302.05206)  
- [Stanford Alpaca: An Instruction-following LLaMA model](https://github.com/tatsu-lab/stanford_alpaca)  
- [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)  
- [Fine-Tune Alpaca in Arabic](https://www.linkedin.com/posts/yassine-boukhari-006748217_alpaca-a-strong-replicable-instruction-following-activity-7043223149710036992-YUJb?utm_source=share&utm_medium=member_android)  
- [TRL: Transformer Reinforcement Learning](https://github.com/lvwerra/trl)  
- [Large Language Model (LLM) Primers Tutorial](https://www.linkedin.com/posts/amanc_artificialintelligence-machinelearning-ai-activity-7045245910850695168-Fp9K/?utm_source=share&utm_medium=member_android) [Great]  
- [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  
- [Microsoft JARVIS & HuggingGPT](https://github.com/microsoft/JARVIS) [Interesting]  
- [open-source LLMs](https://www.linkedin.com/posts/sahar-mor_artificialintelligence-machinelearning-activity-7049789761728770049-QLsv/?utm_source=share&utm_medium=member_android)  
- [GPT4Free](https://github.com/xtekky/gpt4free)  
- [HuggingChat](https://huggingface.co/chat/)  
- [LaMini-LM: A Diverse Herd of Distilled Models](https://github.com/mbzuai-nlp/LaMini-LM/)  
- [RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset](https://github.com/togethercomputer/RedPajama-Data)  
- [BigCode](https://huggingface.co/bigcode)  
- [OpenLLaMA](https://github.com/openlm-research/open_llama)
- [Dromedary: towards helpful, ethical and reliable LLMs](https://github.com/IBM/Dromedary)  
- [MPT-7B Model with Commercial Licence](https://huggingface.co/mosaicml/mpt-7b/blob/main/README.md)  
- [MPT-7B Story Writer](https://huggingface.co/mosaicml/mpt-7b-storywriter)  
- [MPT-7B](https://github.com/mosaicml/llm-foundry)  
- [MPT-7B Blog](https://www.mosaicml.com/blog/mpt-7b)  
- [Open LLMs](https://github.com/eugeneyan/open-llms)  
- [Google PaLM 2](https://ai.google/discover/palm2)  
- [BLOOMChat](https://github.com/sambanova/bloomchat)
- [LLMs Practical Guide](https://github.com/Mooler0410/LLMsPracticalGuide)
- [FrugalGPT](https://www.linkedin.com/posts/sanyambhutani_saving-98-llm-usage-costs-stanford-activity-7062420577357037568-t0a8/?utm_source=share&utm_medium=member_android)  
- [ChatALL](https://github.com/sunner/ChatALL) [Great]   
- [Falcon LLM](https://falconllm.tii.ae/)  
- [The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon) [Great]  
- [Open LLMs](https://github.com/eugeneyan/open-llms) [Great]  

### 100K Tokens LLMs:
- [Claude LLM](https://www.linkedin.com/posts/itamar-g1_anthropic-openais-biggest-rivalry-just-activity-7063773334831775744-cQ4L/?utm_source=share&utm_medium=member_android)
- [Some Notes about the 100K Claude LLM Model](https://www.linkedin.com/posts/sahar-mor_claude-a-gpt-competitor-from-anthropic-activity-7062811160168841216-z4u9/?utm_source=share&utm_medium=member_android)  

### Frameworks for Training & Using Large Language Models (LLMs):
- [ColossalAI: Library for LLMs](https://github.com/hpcaitech/ColossalAI)  
- [LangChain: Library for Building applications with LLMs](https://github.com/hwchase17/langchain)  
- [LangChain Chat](https://github.com/hwchase17/chat-langchain) 
- [LangChain Crash Course](https://www.youtube.com/watch?v=LbT1yp6quS8)  
- [LangChain 101](https://www.linkedin.com/posts/munjal-patel_llm-chatgpt-machinelearning-activity-7049757220300800000-hH7I/?utm_source=share&utm_medium=member_android)  
- [LangChain Resources](https://www.linkedin.com/posts/sonali-pattnaik_generativeai-ai-activity-7063160223967973376-3K0P/?utm_source=share&utm_medium=member_android)  
- [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain)  
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)  
- [Deepset Haystack Framework](https://github.com/deepset-ai/haystack)  
- [LMQL: A query language for programming LLMs](https://github.com/eth-sri/lmql)  
- [LLM Training Frameworks List](https://www.linkedin.com/posts/aboniasojasingarayar_llm-gpt3-framework-activity-7047449940192591872-3VYc/?utm_source=share&utm_medium=member_android)  
- [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)  
- [Lamini: The LLM engine for rapidly customizing models](https://github.com/lamini-ai/lamini)  
- [Scikit-LLM: Sklearn Meets Large Language Models](https://github.com/iryna-kondr/scikit-llm)  
- [Chainlit](https://github.com/Chainlit/chainlit)
- [LangKit: an open-source text metrics toolkit for monitoring language models](https://github.com/whylabs/langkit)  
- [HuggingFace Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents)
- [privateGPT: Ask questions to your documents using the power of LLMs](https://github.com/imartinez/privateGPT)  
- [Spacy LLM](https://github.com/explosion/spacy-llm)  

### LLMs Courses & Tutorials:
- [LLM Bootcamp - Spring 2023](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)  
- [LLM University](https://docs.cohere.com/docs/llmu)  

### LLMs Ranking:
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)  
- [Chatbot Arena Leaderboard](https://lmsys.org/blog/2023-05-10-leaderboard/)  

### Building NLP Applications Powered by LLMs (Different Methods for Augmenting Knowledge to LLMs):
- [Ask a Book Questions with LangChain OpenAI](https://bennycheung.github.io/ask-a-book-questions-with-langchain-openai) [Great]  
- [OpenAI Web QA Embeddings](https://platform.openai.com/docs/tutorials/web-qa-embeddings)  
- [Deepset Haystack Framework](https://github.com/deepset-ai/haystack)  
- [Stanford Retrieval-based NLP](https://ai.stanford.edu/blog/retrieval-based-NLP/)  
- [Hypothetical Document Embeddings (HyDE)](https://www.linkedin.com/posts/activity-7048838677438861312-8MFD/?utm_source=share&utm_medium=member_android)  
- [ChatDB: Augmenting LLMs with Databases](https://chatdatabase.github.io/)
- [ChatNode](https://www.chatnode.ai/)  
- [Emerging Architectures for LLM Applications](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/)  

### Vector Database Libraries:
- [weaviate](https://weaviate.io/)  
- [weaviate GitHub](https://github.com/weaviate/weaviate)  
- [chroma](https://github.com/chroma-core/chroma)  
- [Qdrant: Vector Database for AI Applications](https://github.com/qdrant/qdrant)  
- [pinecone](https://www.pinecone.io/)  
- [rektor-db](https://github.com/codediodeio/rektor-db)  
- [pgvector](https://github.com/pgvector/pgvector)  
- [LlamaIndex: comprehensive toolkit to perform data augmentation for LLMs](https://github.com/jerryjliu/llama_index)  

### Training & Using Large Language Models (LLMs) on Low Resource Machines:
- [Cramming: Training a Language Model on a Single GPU in One Day](https://github.com/jonasgeiping/cramming) [**Great**]  
- [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft) [**Great**]    
- [PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft) [**Great**]   
- [PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft) [**Great**]  
- [Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)  
- [bitsandbytes: 8-bit CUDA functions for PyTorch](https://github.com/TimDettmers/bitsandbytes)  
- [Alpaca-LoRA: Low-Rank LLaMA Instruct-Tuning on consumer hardware](https://github.com/tloen/alpaca-lora) [Great]  
- [LLaMA & Alpaca Tutorial: “ChatGPT” On Your Local Computer](https://medium.com/@martin-thissen/llama-alpaca-chatgpt-on-your-local-computer-tutorial-17adda704c23)  
- [Dalai: The simplest way to run LLaMA on your local machine](https://github.com/cocktailpeanut/dalai)  
- [pyllama](https://github.com/juncongmoo/pyllama)  
- [Alpaca-LoRA-Serve](https://github.com/deep-diver/Alpaca-LoRA-Serve)  
- [llama.cpp: Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)  
- [alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)  
- [SparseGPT: Remove 100 Billion Parameters of LLMs](https://neuralmagic.com/blog/sparsegpt-remove-100-billion-parameters-for-free/)  
- [xFormers: Toolbox to Accelerate Research on Transformers](https://github.com/facebookresearch/xformers)  
- [LLaMA-Adapter: Efficient Fine-tuning of LLaMA (Fine-tuning LLaMA to follow instructions within 1 Hour and 1.2M Parameters)](https://github.com/ZrrSkywalker/LLaMA-Adapter)  
- [GPT4All](https://github.com/nomic-ai/gpt4all) [Great]  
- [Vicuna web page](https://vicuna.lmsys.org/) [Great]   
- [Vicuna GitHub: FastChat](https://github.com/lm-sys/FastChat)  
- [PetGPT](https://github.com/maziarraissi/PetGPT)  
- [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)  
- [baize Chatbot](https://github.com/project-baize/baize-chatbot)  
- [Koala](https://github.com/young-geng/EasyLM#koala)  
- [Gorilla: An API store for LLMs](https://github.com/ShishirPatil/gorilla)  
- [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)  
- [Auto-GPT](https://github.com/Torantulino/Auto-GPT)  
- [xTuring](https://github.com/stochasticai/xTuring)  
- [GPTCache](https://github.com/zilliztech/gptcache)  
- [Dolly-v2-12B](https://huggingface.co/databricks/dolly-v2-12b)  
- [Web LLM](https://github.com/mlc-ai/web-llm)  
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora)  
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq)  

### LLMs on Mobile Devices:
- [MLC LLM](https://github.com/mlc-ai/mlc-llm)  

### LLM Applications & APIs:
- [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)  
- [Bard API](https://github.com/dsdanielpark/Bard-API)  

### Prompt Engineering:
- [Different Kinds of Prompt Engineering](https://www.linkedin.com/posts/munjal-patel_generativeai-largelanguagemodels-llm-activity-7051862874935197696-2E_J/?utm_source=share&utm_medium=member_android)  
- [Prompt Engineering Guide](https://www.promptingguide.ai/)  

### LLM-based Recommender Systems:
- [ChatGPT-based Recommender Systems](https://blog.reachsumit.com/posts/2023/05/chatgpt-for-recsys/)  

### LLM Data Sets:
- [SlimPajama: A 627B token cleaned and deduplicated version of RedPajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)  

### Excellent & Easy to Learn Resources for Learning Transformers:
- [e2eml transformers from scratch](https://e2eml.school/transformers.html) [**Excellent**]  
- [annotated-transformer: Learning transformers from code](http://nlp.seas.harvard.edu/annotated-transformer/#a-first-example)  
- [Transformers Recipe](https://github.com/dair-ai/Transformers-Recipe)  

### Persian based Transformer Models:
- [ALBERT-Persian](https://github.com/m3hrdadfi/albert-persian)  
- [ALBERT-Persian Demo Page](https://albert-lab.m3hrdadfi.me/)  
- [ALBERT-Farsi-base-v2 in HuggingFace](https://huggingface.co/m3hrdadfi/albert-fa-base-v2)  
- [ParsBERT - Model for Persian Language Understanding](https://github.com/hooshvare/parsbert)  
- [ARMAN](https://github.com/alirezasalemi7/ARMAN) [Great]   
- [ParsBigBird: Persian Bert For Long-Range Sequences](https://github.com/sajjjadayobi/ParsBigBird) [Great]    
- [PersianQA](https://github.com/sajjjadayobi/PersianQA)   
- [Persian (Farsi) Pre-trained Language Models](https://nlpdataset.ir/farsi/pre-trained_lm.html) [Great]  

## Transfer Learning with Transformers:
- [Transfer Learning for NLP via BERT for Text Classification](https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/)  
- [Text Classification with BERT Tokenizer](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/)   
- [Bert Text Classification](https://github.com/Shivampanwar/Bert-text-classification)  
- [Persian Semantic Search](https://github.com/m3hrdadfi/semantic-search)  
- [Toward fine-tuning a state of the art Natural Language Inference (NLI) model for Persian](https://haddadhesam.medium.com/toward-fine-tuning-a-state-of-the-art-natural-language-inference-nli-model-for-persian-4d538ea4525d)  

### Siamese Netowrks and Dual BERT for Multi Text Classification:  
- [Siamese and Dual BERT for Multi-text Classification](https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533)    
- [Transfer Learning via Siamese Networks](https://www.inovex.de/blog/transfer-learning-siamese-networks/)  

## Attention Mechanism:
- [Attention Mechanism](https://blog.floydhub.com/attention-mechanism/)  
- [Visualizing A Neural Machine Translation Model - Attention Mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  
- [Intuitive Understanding of Attention Mechanism in Deep Learning](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)  
- [Structured Attention Networks](https://medium.com/uci-nlp/summary-structured-attention-networks-f1917dd622af)  

## Sequence Modeling:
- [WaveNet: Increasing reception field using dilated convolution](https://medium.com/@kion.kim/wavenet-a-network-good-to-know-7caaae735435)  
- [Understanding WaveNet architecture](https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623)  
- [WaveNet: A Generative Model for Raw Audio](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/wavenet-a-generative-model-for-raw-audio-84b2aa5fb4a0)  
- [How WaveNet Works](https://towardsdatascience.com/how-wavenet-works-12e2420ef386)  
- [PyTorch Tutorial to Sequence Labeling](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)  

## Text Summarization:
- [Bert Extractive Summarizer](https://pypi.org/project/bert-extractive-summarizer/) [**Great**]   
- [Generating Text Summaries Using GPT-2 on PyTorch with Minimal Training](https://blog.paperspace.com/generating-text-summaries-gpt-2/) [_Good_]    
- [A Gentle Introduction to Text Summarization in Machine Learning](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)  
- [Taming Recurrent Neural Networks for Better Summarization](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html)  
- [PyTorch implementation of "Get to the point"](https://github.com/mjc92/GetToThePoint)  
- [TensorFlow implementation of "Get to the point"](https://github.com/abisee/pointer-generator)  

## Language Model:
- [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/)   
- [D2L: Language Models and Dataset](https://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)  
- [Develop a word-level Neural Language Model in Keras](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)  
- [IBM deep learning language model](https://github.com/IBM/deep-learning-language-model)  
- [BERT language model](https://devopedia.org/bert-language-model)  
- [Facebook AI: GSLM](https://www.marktechpost.com/2021/09/09/facebook-ai-introduces-gslm-generative-spoken-language-model-a-textless-nlp-model-that-breaks-free-completely-of-the-dependence-on-text-for-training/)   
- [Language Modeling Great Tutorial](https://lena-voita.github.io/nlp_course/language_modeling.html)   
- [GALACTICA: general-purpose scientific language model](https://github.com/paperswithcode/galai) [Great]  
- [Distributed Training of Language Models with Reinforcement Learning via Human Feedback (RLHF)](https://github.com/CarperAI/trlx) [**Excellent**]  

## Text & Document Classification:
- [hedwig - PyTorch deep learning models for document classification](https://github.com/castorini/hedwig)  

## Topic Modeling:
- [Topic Modeling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)  

## Sentiment Analysis:
- [Introduction to Deep Learning – Sentiment Analysis](https://nlpforhackers.io/deep-learning-introduction/)  

## Co-Reference Resolution:
- [Coreference Resolution for Chatbots](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30)  
- [Hugging Face - CoRef](https://huggingface.co/coref/)  

## Imbalance Handling in NLP:
- [Over-Sampling using SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) [_SMOTE for high-dimensional class-imbalanced data_]  
- [Over-sampling via imbalanced-learn library](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)  
- [Imbalanced Data Handling](https://www.jeremyjordan.me/imbalanced-data/)  

## Information Retrieval:
- [PyTerrier: Python API for Terrier](https://github.com/terrier-org/pyterrier)  

## Distance Measures:
- [Edit Distance](https://www.geeksforgeeks.org/edit-distance-dp-5/)  

## Text-based Emotion Recognition:
- [XLM-EMO: Multilingual Emotion Prediction in Social Media Text](https://github.com/MilaNLProc/xlm-emo)  

## Chatbot:
- [Rasa Chatbot](https://github.com/RasaHQ/rasa) [**Great**]      
- [Learn how to Build and Deploy a Chatbot in Minutes using Rasa](https://www.analyticsvidhya.com/blog/2019/04/learn-build-chatbot-rasa-nlp-ipl/)   
- [chatbot with DialoGPT](https://www.machinecurve.com/index.php/2021/03/16/easy-chatbot-with-dialogpt-machine-learning-and-huggingface-transformers/)   
- [DialoGPT: huggingface Transformer](https://huggingface.co/transformers/model_doc/dialogpt.html)   
- [deeppavlov](https://github.com/deeppavlov/DeepPavlov) [**Great**]  
- [PyTorch Chatbot Tutorial](https://brsoff.github.io/tutorials/beginner/chatbot_tutorial.html)   
- [Implement a Simple Chat Bot With PyTorch](https://www.python-engineer.com/posts/chatbot-pytorch/)   
- [GPT2 Chatbot PyTorch](https://github.com/devjwsong/gpt2-chatbot-pytorch)   
- [PyTorch Official Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)    
- [PaddlePaddle Knover: toolkit for knowledge grounded dialogue generation](https://github.com/PaddlePaddle/Knover)   
- [PaddlePaddle PLATO-2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/dialogue/plato-2)   
- [ParlAI](https://github.com/facebookresearch/ParlAI) [Great]     
- [huggingface: Transformers](https://github.com/huggingface/transformers) [Great]  
- [huggingface: Blenderbot](https://huggingface.co/transformers/model_doc/blenderbot.html) [**Great**]  
- [huggingface: Blenderbot Small](https://huggingface.co/transformers/model_doc/blenderbot_small.html) [**Great**]  
- [huggingface: GPT-2 Text Generation](https://huggingface.co/gpt2?text=A+long+time+ago%2C) [**Great**]            
- [Seq2seq Chatbot](https://github.com/ricsinaruto/Seq2seqChatbots)   
- [seq2seq Chatbot implemented in Pytorch](https://github.com/khordoo/chatbot-pytorch)   
- [papers with code: chatbot](https://paperswithcode.com/task/chatbot)   
- [Proudly Leading the Chatbot](https://www.analyticsinsight.net/ankush-sabharwal-proudly-leading-the-chatbot-sphere-with-strategical-innovations-and-implementations/)  
- [Real Python: Build a Chatbot with Python ChatterBot](https://realpython.com/build-a-chatbot-python-chatterbot/)  
- [A step-by-step guide to building a chatbot based on your own documents with GPT](https://bootcamp.uxdesign.cc/a-step-by-step-guide-to-building-a-chatbot-based-on-your-own-documents-with-gpt-2d550534eea5)  

### Chatbot Evaluation Metrics:
- [Chatbot Analytics: 9 Key Metrics](https://www.tidio.com/blog/chatbot-analytics/)  
- [Chatbot Statistics for 2023](https://www.tidio.com/blog/chatbot-statistics/)  
- [Chatbot Analytics 101: Essential Metrics to Track](https://blog.hootsuite.com/chatbot-analytics/)  
- [12 Metrics For Chatbot Analytics](https://www.kommunicate.io/blog/metrics-for-chatbot-analytics/)  
- [ParlAI Evaluation Metrics for Chatbot](https://github.com/facebookresearch/ParlAI/blob/14a10258bf90218341e0253d1c5a88c9d2cd013f/docs/source/tutorial_metrics.md)  
- [Chatbot Evaluation Metrics](https://github.com/ahkarami/Great-Deep-Learning-Tutorials/blob/master/NLP/Chatbot_Evaluation_Metrics.md) [**Great**]  

### OpenAI ChatGPT & Its Applications:  
- [OpenAI ChatGPT](https://openai.com/blog/chatgpt/) [Amazing]  
- [Description of How OpenAI ChatGPT Works: Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://github.com/huggingface/blog/blob/main/rlhf.md)  
- [How ChatGPT was Trained](https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-chatgpt-activity-7007019154666909696-T5WM/?utm_source=share&utm_medium=member_android)  
- [ChatGPT Android SDK](https://github.com/skydoves/chatgpt-android/releases)  
- [ChatGPT awesome apps](https://www.linkedin.com/posts/tarrysingh_chatgpt-activity-7017947289721655296-7-pK/?utm_source=share&utm_medium=member_android)  
- [A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)  
- [Is ChatGPT a General-Purpose Natural Language Processing Task Solver?](https://arxiv.org/abs/2302.06476)  
- [aman.ai chatGPT Tutorial](https://aman.ai/primers/ai/chatGPT/) [Great]  
- [ChatGPT for customer service](https://www.intercom.com/ai-bot)  
- [Chatgpt Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin)  
- [Trending AI Tools](https://galionaitools.blogspot.com/2023/03/trending-ai-tools.html)  
- [Merlin: OpenAI ChatGPT Plus extension on all websites](https://merlin.foyer.work/)  
- [Adrenaline](https://useadrenaline.com/app)  
- [Using LLMs as agents that orchestrate tools](https://www.linkedin.com/posts/moritz-laurer_augmented-language-models-a-survey-activity-7047924951625953281-0XDj/?utm_source=share&utm_medium=member_android) [Interesting]  
- [ChatGPT API Using Python](https://www.machinelearning-basics.com/2023/04/chatgpt-api-using-python.html?m=1)  
- [parthean: A Startup about Financial Expert via ChatGPT](https://www.parthean.com/)  
- [Notes on the cost of ChatGPT](https://www.linkedin.com/posts/laurencevanelegem_sam-altman-ceo-of-openai-dropped-a-at-activity-7061987804548870144-RF9y/?utm_source=share&utm_medium=member_android)  
- [Ortus - your YouTube AI buddy](https://chrome.google.com/webstore/detail/ortus-your-youtube-ai-bud/jmpepfdhkjkknfpnfohnmnjoceepcbmp)  

## NLP Programming Notes:
- [100 Times Faster Natural Language Processing in Python](https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced)  
- [Multi-label Text Classification using BERT](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)  
- [Learning Meaning in Natural Language Processing](https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e)  
- [Train and Deploy the Mighty Transformer NLP models using FastBert and AWS SageMaker](https://medium.com/@kaushaltrivedi/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3)  
- [Distilling knowledge from Neural Networks to build smaller and faster models](https://blog.floydhub.com/knowledge-distillation/)  
- [HarfBuzz - a text shaping library](https://github.com/harfbuzz/harfbuzz) [_Useful_]  
- [PruneBERT - Hugging Face](https://github.com/huggingface/transformers/tree/master/examples/movement-pruning)  
- [spacy-streamlit: spaCy building blocks for Streamlit apps](https://github.com/explosion/spacy-streamlit)  
- [HuggingFace Evaluate Library](https://github.com/huggingface/evaluate)  
- [NeMo - toolkit for Conversational AI](https://github.com/NVIDIA/NeMo) [_Excellent_]  

## Data Annotation Tools:
- [doccano is an open source text annotation tool](https://github.com/doccano/doccano) [**Great**]  
- [doccano-divar](https://doccano.divar.ir/)  

## NLP Courses:
- [HuggingFace Course](https://github.com/huggingface/course)  
- [NLP Zero to One: Full Course](https://medium.com/nerd-for-tech/nlp-zero-to-one-full-course-4f8e1902c379)  
- [Stanford CS25: Transformers United](https://web.stanford.edu/class/cs25/)  

## Other NLP Topics:
- [HybridNLP - Tutorial on Hybrid Techniques for Knowledge-based NLP](https://github.com/hybridnlp/tutorial)  
- [Top 10 GPT-3 Tools Easing Content Creation Work in 2022](https://www.analyticsinsight.net/top-10-gpt-3-tools-easing-content-creation-work-in-2022/) [Interesting]  
