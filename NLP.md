# Great Deep Learning Tutorials for Natural Language Processing (NLP)
A Great Collection of Deep Learning Tutorials and Repositories for Natural Language Processing (NLP)

## General:
- [Great NLP Posts](http://jalammar.github.io/)  
- [Awesome NLP Paper Discussions - Hugging Face](https://github.com/huggingface/awesome-papers) [_Excellent_]  
- [Ten trends in Deep learning NLP](https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/)  
- [Attention in RNNs](https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05)  
- [BERT - TensorFlow](https://github.com/google-research/bert)  
- [Understanding XLNet](https://www.borealisai.com/en/blog/understanding-xlnet/)  
- [XLNet - TensorFlow](https://github.com/zihangdai/xlnet)  
- [XLM (PyTorch implementation of Cross-lingual Language Model Pretraining)](https://github.com/facebookresearch/XLM)  
- [Pretrained PyTorch models for BERT](https://github.com/huggingface/pytorch-pretrained-BERT)  
- [Library of state-of-the-art pretrained models for NLP](https://github.com/huggingface/pytorch-transformers#quick-tour) [_Excellent_]  
- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5)  
- [PyTorch Hub - BERT](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)  
- [A Simple Guide On Using BERT for Binary Text Classification](https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04)  
- [Core ML 3 implementation of BERT for Question answering](https://github.com/huggingface/swift-coreml-transformers)  
- [NLP - Keras - Intro](https://nlpforhackers.io/keras-intro/)  
- [AllenNLP](https://allennlp.org/)  [_General NLP_]
- [Stanza - A Python NLP Library for Many Human Languages](https://stanfordnlp.github.io/stanza/)  
- [The Best NLP Papers From ICLR 2020](https://www.topbots.com/best-nlp-papers-from-iclr-2020)  
- [Deep learning for natural language processing and information retrieval at the University of Waterloo](https://github.com/castorini)  
- [Natural Language Processing With spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)  [_Great_]  
- [NLP Papers](https://github.com/AliAkbarBadri/nlp-papers)   
- [A Great NLP Course](https://lena-voita.github.io/nlp_course.html)  
- [KerasNLP: Modular NLP Workflows for Keras](https://github.com/keras-team/keras-nlp)  

## General Persian based libraries:
- [Parsivar: library for Persian text preprocessing](https://github.com/ICTRC/Parsivar)   
- [Hazm](https://github.com/sobhe/hazm)  
- [persianNLP](https://github.com/persiannlp)  
- [ParsiNLU: Comprehensive suit of high-level NLP tasks for Persian language](https://github.com/persiannlp/parsinlu)   
- [FarsTail: A Persian Natural Language Inference Dataset](https://github.com/dml-qom/FarsTail)  
- [wordfreq: Access a database of word frequencies](https://github.com/rspeer/wordfreq)  
- [Persian Stop Words List](https://github.com/kharazi/persian-stopwords)  
- [Persian Stop Words List in Hazm Repo](https://github.com/sobhe/hazm/blob/master/hazm/data/stopwords.dat)  

## Text Representation:
- [Beyond Word Embeddings Part 1](https://towardsdatascience.com/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f)  
- [Beyond Word Embeddings Part 2](https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec)  
- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)  
- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)  
- [Word Embedding](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)  
- [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)  
- [Introduction to Word Vectors](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf) 
- [Word2vec Made Easy](https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae)  
- [What is GloVe? Part I](https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970)  
- [What is GloVe? Part II](https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0)  
- [What is GloVe? Part III](https://towardsdatascience.com/emnlp-what-is-glove-part-iii-c6090bed114)  
- [What is GloVe? Part IV](https://towardsdatascience.com/emnlp-what-is-glove-part-iv-e605a4c407c8)  
- [What is GloVe? Part V](https://towardsdatascience.com/emnlp-what-is-glove-part-v-fa888272c290)  
- [ELMo: Deep Contextualized Word Representation](https://allennlp.org/elmo)  
- [A Step-by-Step NLP Guide to Learn ELMo](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)  
- [ELMo: Contextual language embedding](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)  
- [word embeddings with ELMo](https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045)  
- [Doc2Vec - Gensim](https://radimrehurek.com/gensim/models/doc2vec.html)  

## Self-Supervised Learning in NLP:
- [https://amitness.com/2020/05/self-supervised-learning-nlp/](https://amitness.com/2020/05/self-supervised-learning-nlp/)  
- [COSINE: Fine-Tuning Pre-trained Language Model with Weak Supervision](https://github.com/yueyu1030/COSINE)  

## RNN, LSTM, and GRU:
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- [Illustrated Guide to LSTM’s and GRU’s](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)  
- [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)  
- [Recurrent Neural Networks and LSTM explained](https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9)  
- [Long Short-Term Memory (LSTM): Concept](https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359)  
- [Understanding architecture of LSTM cell from scratch](https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4)  
- [Basic understanding of LSTM](https://blog.goodaudience.com/basic-understanding-of-lstm-539f3b013f1e)  
- [Taming LSTMs with PyTorch](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e)  
- [Introduction to LSTM](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_medium=ELMoNLParticle&utm_source=blog)  
- [Introduction to RNNs](https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/)  

## Transformers:
- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)  
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  
- [What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)  
- [How Transformers work in deep learning and NLP](https://theaisummer.com/transformer/)    
- [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)  
- [How do Transformers Work in NLP?](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)  
- [The Essence of Transformers](https://towardsdatascience.com/the-essence-of-transformers-9fb8e14cc465) [Good]    
- [BERT for Dummies](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)  
- [The Dark Secrets of BERT](https://text-machine-lab.github.io/blog/2020/bert-secrets/)  
- [A Survey of Long-Term Context in Transformers](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/) [_Great_]  
- [The Transformer Family](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)  
- [The Transformer Isn’t As Hard To Understand As You Might Think](https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8)  
- [Review of Compact Transformer Architectures](https://medium.com/@jfd2139/review-of-compact-transformer-architectures-c477b797e2d5) [**Great**]   
- [REFORMER: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)  
- [GPT-3: Language Models are Few-Shot Learners](https://github.com/openai/gpt-3)  
- [GPT-3 Sandbox](https://github.com/shreyashankar/gpt3-sandbox)  
- [Regular Expressions (Regex) Generated by GPT-3](https://losslesshq.com/)  
- [Auto Regex: Converting English description to Regex](https://www.autoregex.xyz/) [Good]  
- [minGPT](https://github.com/karpathy/minGPT)  
- [NVIDIA FasterTransformer: Transformer related optimization, including BERT & GPT](https://github.com/NVIDIA/FasterTransformer)  
- [OpenNMT CTranslate2: Fast inference engine for Transformer models](https://github.com/OpenNMT/CTranslate2/)  
- [Deploying GPT-J and T5 with FasterTransformer and Triton Inference Server](https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/?ncid=so-link-499508#cid=dl05_so-link_en-us) [Interesting]  
- [MEND: Fast Model Editing at Scale](https://github.com/eric-mitchell/mend) [**Excellent Work**]   
- [BorealisAI Transformers I: Introduction](https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/)  
- [OpenAI Best Practices for Deploying Language Models](https://openai.com/blog/best-practices-for-deploying-language-models/)  
- [OPT-IML](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML)  

### Large Language Models (LLMs):
- [LLaMA](https://github.com/facebookresearch/llama)  
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761) [Great]  
- [Toolformer GitHub](https://github.com/lucidrains/toolformer-pytorch)  
- [Amazon Multimodal Chain-of-Thought Reasoning in Language Models](https://github.com/amazon-science/mm-cot)  
- [LLaMA-based ChatGPT Training](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama) [Great]  
- [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/abs/2302.05206)  

### Training & Using Large Language Models (LLMs) in Low Resource Machines:
- [Cramming: Training a Language Model on a Single GPU in One Day](https://github.com/jonasgeiping/cramming) [**Great**]  
- [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft) [**Great**]    

### Excellent & Easy to Learn Resources for Learning Transformers:
- [e2eml transformers from scratch](https://e2eml.school/transformers.html) [**Excellent**]  
- [annotated-transformer: Learning transformers from code](http://nlp.seas.harvard.edu/annotated-transformer/#a-first-example)  
- [Transformers Recipe](https://github.com/dair-ai/Transformers-Recipe)  

### Persian based Transformer Models:
- [ALBERT-Persian](https://github.com/m3hrdadfi/albert-persian)  
- [ALBERT-Persian Demo Page](https://albert-lab.m3hrdadfi.me/)  
- [ALBERT-Farsi-base-v2 in HuggingFace](https://huggingface.co/m3hrdadfi/albert-fa-base-v2)  
- [ParsBERT - Model for Persian Language Understanding](https://github.com/hooshvare/parsbert)  
- [ARMAN](https://github.com/alirezasalemi7/ARMAN) [Great]   
- [ParsBigBird: Persian Bert For Long-Range Sequences](https://github.com/sajjjadayobi/ParsBigBird) [Great]    
- [PersianQA](https://github.com/sajjjadayobi/PersianQA)   

## Transfer Learning with Transformers:
- [Transfer Learning for NLP via BERT for Text Classification](https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/)  
- [Text Classification with BERT Tokenizer](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/)   
- [Bert Text Classification](https://github.com/Shivampanwar/Bert-text-classification)  
- [Persian Semantic Search](https://github.com/m3hrdadfi/semantic-search)  
- [Toward fine-tuning a state of the art Natural Language Inference (NLI) model for Persian](https://haddadhesam.medium.com/toward-fine-tuning-a-state-of-the-art-natural-language-inference-nli-model-for-persian-4d538ea4525d)  

### Siamese Netowrks and Dual BERT for Multi Text Classification:  
- [Siamese and Dual BERT for Multi-text Classification](https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533)    
- [Transfer Learning via Siamese Networks](https://www.inovex.de/blog/transfer-learning-siamese-networks/)  

## Attention Mechanism:
- [Attention Mechanism](https://blog.floydhub.com/attention-mechanism/)  
- [Visualizing A Neural Machine Translation Model - Attention Mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  
- [Intuitive Understanding of Attention Mechanism in Deep Learning](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)  
- [Structured Attention Networks](https://medium.com/uci-nlp/summary-structured-attention-networks-f1917dd622af)  

## Sequence Modeling:
- [WaveNet: Increasing reception field using dilated convolution](https://medium.com/@kion.kim/wavenet-a-network-good-to-know-7caaae735435)  
- [Understanding WaveNet architecture](https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623)  
- [WaveNet: A Generative Model for Raw Audio](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/wavenet-a-generative-model-for-raw-audio-84b2aa5fb4a0)  
- [How WaveNet Works](https://towardsdatascience.com/how-wavenet-works-12e2420ef386)  
- [PyTorch Tutorial to Sequence Labeling](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)  

## Text Summarization:
- [Bert Extractive Summarizer](https://pypi.org/project/bert-extractive-summarizer/) [**Great**]   
- [Generating Text Summaries Using GPT-2 on PyTorch with Minimal Training](https://blog.paperspace.com/generating-text-summaries-gpt-2/) [_Good_]    
- [A Gentle Introduction to Text Summarization in Machine Learning](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)  
- [Taming Recurrent Neural Networks for Better Summarization](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html)  
- [PyTorch implementation of "Get to the point"](https://github.com/mjc92/GetToThePoint)  
- [TensorFlow implementation of "Get to the point"](https://github.com/abisee/pointer-generator)  

## Language Model:
- [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/)   
- [D2L: Language Models and Dataset](https://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)  
- [Develop a word-level Neural Language Model in Keras](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)  
- [IBM deep learning language model](https://github.com/IBM/deep-learning-language-model)  
- [BERT language model](https://devopedia.org/bert-language-model)  
- [Facebook AI: GSLM](https://www.marktechpost.com/2021/09/09/facebook-ai-introduces-gslm-generative-spoken-language-model-a-textless-nlp-model-that-breaks-free-completely-of-the-dependence-on-text-for-training/)   
- [Language Modeling Great Tutorial](https://lena-voita.github.io/nlp_course/language_modeling.html)   
- [GALACTICA: general-purpose scientific language model](https://github.com/paperswithcode/galai) [Great]  
- [Distributed Training of Language Models with Reinforcement Learning via Human Feedback (RLHF)](https://github.com/CarperAI/trlx) [**Excellent**]  

## Text & Document Classification:
- [hedwig - PyTorch deep learning models for document classification](https://github.com/castorini/hedwig)  

## Topic Modeling:
- [Topic Modeling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)  

## Sentiment Analysis:
- [Introduction to Deep Learning – Sentiment Analysis](https://nlpforhackers.io/deep-learning-introduction/)  

## Co-Reference Resolution:
- [Coreference Resolution for Chatbots](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30)  
- [Hugging Face - CoRef](https://huggingface.co/coref/)  

## Imbalance Handling in NLP:
- [Over-Sampling using SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) [_SMOTE for high-dimensional class-imbalanced data_]  
- [Over-sampling via imbalanced-learn library](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)  
- [Imbalanced Data Handling](https://www.jeremyjordan.me/imbalanced-data/)  

## Information Retrieval:
- [PyTerrier: Python API for Terrier](https://github.com/terrier-org/pyterrier)  

## Distance Measures:
- [Edit Distance](https://www.geeksforgeeks.org/edit-distance-dp-5/)  

## Text-based Emotion Recognition:
- [XLM-EMO: Multilingual Emotion Prediction in Social Media Text](https://github.com/MilaNLProc/xlm-emo)  

## Chatbot:
- [Rasa Chatbot](https://github.com/RasaHQ/rasa) [**Great**]      
- [Learn how to Build and Deploy a Chatbot in Minutes using Rasa](https://www.analyticsvidhya.com/blog/2019/04/learn-build-chatbot-rasa-nlp-ipl/)   
- [chatbot with DialoGPT](https://www.machinecurve.com/index.php/2021/03/16/easy-chatbot-with-dialogpt-machine-learning-and-huggingface-transformers/)   
- [DialoGPT: huggingface Transformer](https://huggingface.co/transformers/model_doc/dialogpt.html)   
- [deeppavlov](https://github.com/deeppavlov/DeepPavlov) [**Great**]  
- [PyTorch Chatbot Tutorial](https://brsoff.github.io/tutorials/beginner/chatbot_tutorial.html)   
- [Implement a Simple Chat Bot With PyTorch](https://www.python-engineer.com/posts/chatbot-pytorch/)   
- [GPT2 Chatbot PyTorch](https://github.com/devjwsong/gpt2-chatbot-pytorch)   
- [PyTorch Official Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)    
- [PaddlePaddle Knover: toolkit for knowledge grounded dialogue generation](https://github.com/PaddlePaddle/Knover)   
- [PaddlePaddle PLATO-2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/dialogue/plato-2)   
- [ParlAI](https://github.com/facebookresearch/ParlAI) [Great]     
- [huggingface: Transformers](https://github.com/huggingface/transformers) [Great]  
- [huggingface: Blenderbot](https://huggingface.co/transformers/model_doc/blenderbot.html) [**Great**]  
- [huggingface: Blenderbot Small](https://huggingface.co/transformers/model_doc/blenderbot_small.html) [**Great**]  
- [huggingface: GPT-2 Text Generation](https://huggingface.co/gpt2?text=A+long+time+ago%2C) [**Great**]            
- [Seq2seq Chatbot](https://github.com/ricsinaruto/Seq2seqChatbots)   
- [seq2seq Chatbot implemented in Pytorch](https://github.com/khordoo/chatbot-pytorch)   
- [papers with code: chatbot](https://paperswithcode.com/task/chatbot)   
- [Proudly Leading the Chatbot](https://www.analyticsinsight.net/ankush-sabharwal-proudly-leading-the-chatbot-sphere-with-strategical-innovations-and-implementations/)  
- [Real Python: Build a Chatbot with Python ChatterBot](https://realpython.com/build-a-chatbot-python-chatterbot/)  

### Chatbot Evaluation Metrics:
- [Chatbot Analytics: 9 Key Metrics](https://www.tidio.com/blog/chatbot-analytics/)  
- [Chatbot Statistics for 2023](https://www.tidio.com/blog/chatbot-statistics/)  
- [Chatbot Analytics 101: Essential Metrics to Track](https://blog.hootsuite.com/chatbot-analytics/)  
- [12 Metrics For Chatbot Analytics](https://www.kommunicate.io/blog/metrics-for-chatbot-analytics/)  
- [ParlAI Evaluation Metrics for Chatbot](https://github.com/facebookresearch/ParlAI/blob/14a10258bf90218341e0253d1c5a88c9d2cd013f/docs/source/tutorial_metrics.md)  
- [Chatbot Evaluation Metrics](https://github.com/ahkarami/Great-Deep-Learning-Tutorials/blob/master/NLP/Chatbot_Evaluation_Metrics.md) [**Great**]  

### OpenAI ChatGPT:  
- [OpenAI ChatGPT](https://openai.com/blog/chatgpt/) [Amazing]  
- [Description of How OpenAI ChatGPT Works: Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://github.com/huggingface/blog/blob/main/rlhf.md)  
- [How ChatGPT was Trained](https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-chatgpt-activity-7007019154666909696-T5WM/?utm_source=share&utm_medium=member_android)  
- [ChatGPT Android SDK](https://github.com/skydoves/chatgpt-android/releases)  
- [ChatGPT awesome apps](https://www.linkedin.com/posts/tarrysingh_chatgpt-activity-7017947289721655296-7-pK/?utm_source=share&utm_medium=member_android)  
- [A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)  
- [Is ChatGPT a General-Purpose Natural Language Processing Task Solver?](https://arxiv.org/abs/2302.06476)  

## NLP Programming Notes:
- [100 Times Faster Natural Language Processing in Python](https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced)  
- [Multi-label Text Classification using BERT](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)  
- [Learning Meaning in Natural Language Processing](https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e)  
- [Train and Deploy the Mighty Transformer NLP models using FastBert and AWS SageMaker](https://medium.com/@kaushaltrivedi/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3)  
- [Distilling knowledge from Neural Networks to build smaller and faster models](https://blog.floydhub.com/knowledge-distillation/)  
- [HarfBuzz - a text shaping library](https://github.com/harfbuzz/harfbuzz) [_Useful_]  
- [PruneBERT - Hugging Face](https://github.com/huggingface/transformers/tree/master/examples/movement-pruning)  
- [spacy-streamlit: spaCy building blocks for Streamlit apps](https://github.com/explosion/spacy-streamlit)  
- [HuggingFace Evaluate Library](https://github.com/huggingface/evaluate)  
- [NeMo - toolkit for Conversational AI](https://github.com/NVIDIA/NeMo) [_Excellent_]  

## Data Annotation Tools:
- [doccano is an open source text annotation tool](https://github.com/doccano/doccano) [**Great**]  
- [doccano-divar](https://doccano.divar.ir/)  

## NLP Courses:
- [HuggingFace Course](https://github.com/huggingface/course)  
- [NLP Zero to One: Full Course](https://medium.com/nerd-for-tech/nlp-zero-to-one-full-course-4f8e1902c379)  
- [Stanford CS25: Transformers United](https://web.stanford.edu/class/cs25/)  

## Other NLP Topics:
- [HybridNLP - Tutorial on Hybrid Techniques for Knowledge-based NLP](https://github.com/hybridnlp/tutorial)  
- [Top 10 GPT-3 Tools Easing Content Creation Work in 2022](https://www.analyticsinsight.net/top-10-gpt-3-tools-easing-content-creation-work-in-2022/) [Interesting]  
